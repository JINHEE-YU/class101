# 어플리케이션 성능 테스트
Artillery를 이용하여 GCP에 올려둔 CPU 바운드 애플리케이션 성능 테스트

---
## Artillery 사용 환경 구성
Artillery를 사용하기 위해 Node.js의 설치가 필수이다.

### 1. Node.js 설치 방법
[Node.js공홈](https://nodejs.org/ko/)에서 LTS 버전 설치

### 2. Node.js 설치 확인 방법
CMD 창에서 명령어 node 또는 node -v 입력

### 3. Artillery 설치 방법
[Artillery Docs](https://artillery.io/docs/guides/overview/welcome.html) 참고  
* VSCode의  Terminal창에서 다음 명령어 실행
    > npm install -g artillery
---

## Artillery 사용 방법
1. Test Script 작성 [참고](https://artillery.io/docs/guides/getting-started/writing-your-first-test.html#Load-Phases)
2. terminal에서 script실행
> artillery.cmd run --output report.json ./cpu-test.yam  
또는
artillery run --output report.json cpu-test.yam  
3. report.json 파일을 HTML 파일로 변경
 > artillery.cmd report ./report.json

 ## 팁
 
 VM을 사용하여 cpu 갯수를 늘려서 테스트하면 PM과 달리 ***VM은 인접하고 있는 인스턴스들과 자원을 공유***하기 때문에 항상 똑같은 수준의 퀄리티를 보장할 수 없다.  
 또한 일시적으로 튀어오르는 구간이 생길 수 있다.

 [ 목표 레이턴시 잡는 팁 ]  
 성능 측정 시에는 목표로하는 Latency를 잡아야 한다.
 
 [ 테스트 시 유의사항 ]  
1. 예상 TPS보다 여유롭게 성능 목표치를 잡자 (약 3~4배 높게)
2. API에 기대 Latency를 만족할 때까지 테스트해봐야 한다.
3. Scale-out을 해도 성능이 늘지 않으면 병목을 의심해 보자
 - 단일 요청에 대한 Latency가 몇인지 확인해 보자. 만약 기대치보다 높다면 scale-out으로 해결되지 않는 상황.  
 -- 코드를 수정해야하거나   
 -- 해당 API에서 실행되는 I/O가 병목인 경우  
 -- 네트워크에서 Latency가 발생하는 경우



 ----
## 용어 정리
### Latency
 : 지연시간  
 : HTTP 트랜잭선 시간 ( 1회당 요청 -> 응답 까지 걸리는 시간 )  
 : 0으로 갈수록 HTTP 트랜잭션 1회당 요청 -> 응답까지 짧은 시간안에 왔다는 겁니다. 

당연히 짧으면 짧을수록 좋겠죠?  
하지만 이 시간에는 네트워크 시간 + 애플리케이션에서 처리하는 시간이 포함되어 있기 때문에 0에 가깝게 낮추는 것은 어렵습니다.  
따라서 같은 네트워크 상에 있는 경우 1ms(0.001초)에 가까운 Latency를 가지기도 합니다.

### 그래프의 주요 선 
: max, p95, p50, min 이렇게 4가지 인데요, 각각 이런 의미입니다.
 | 명칭| 설명 |
 |-----|-----|
 | max | 가장 오래 걸린 요청->응답 시간|
 | p95 | 전체 HTTP 트랜잭션 중 가장 빠른것부터 95%까지 (거의 대부분의 트래픽이 여기 포함되겠죠?)|
 | p50 | 전체 HTTP 트랜잭션 중 가장 빠른것부터 50%까지 (절반의 트래픽이 여기 해당됩니다.)|
 | min | 가장 빠르게 온 요청->응답 시간|
 
#### **추가 설명**  
| 명칭 | 설명|
| ---- | ----|
| p99 | 가장 빠른것부터 99%까지의 트래픽에 해당합니다. 거의 모든 트래픽을 의미하는거죠.|

* 실제 벤치마크를 잴 때에도 많이 비교하는게 p99, p95 

* 어플리케이션의 실제 성능 파익의 기준이 되는 것은?  = p99
    *  max를 쓰는게 더 좋지 않나? 라는 생각이 드실 수 있겠지만,   
 사실 네트워크라는건 굉장히 불안정한 존재입니다. 물론 현재, 특히 우리나라의 네트워크 상태는 매우 좋은 편에 속하지만, 그렇다고 하더라도 여러가지 이유로 네트워크상에서 큰 지연시간이 발생하는 경우가 있는데요, 이런게 자주 발생하는 것은 아니고, 드물게 조금씩 발생할 수 있습니다. 하지만 이게 전체 성능 측정 결과에 영향을 미치는게 바람직 하지 않을 수 있겠죠?  
 그래서 가급적 전체 HTTP 트래픽을 대표할 정도의 수준인 p99, p95 정도를 많이 사용합니다.  통계학에서 최상위 몇% 최하위 몇% 등을 제거하고 통계를 내는 것과 비슷한 이유라고 생각해주시면 될 것 같습니다.

----

### 네트워크에 병목이라고 할 수 있는건 사실 2가지로 나눌 수 있습니다.

#### **원인**
Latency와 Bandwidth인데요,

1. Latency는 A와 B 지점 사이에서 패킷이 오고 가는데 얼마나 많은 '시간'이 걸리는지를 의미합니다. 간단히 ping 명령어를 통해 A에서 B로 ping 을 날려서 얼마의 시간이 걸리는지 측정해보면되죠.

2. Bandwidth는 다릅니다. A와 B 지점 사이에 초당 얼마나 많은 '양'의 데이터가 흐를 수 있는가 입니다. 이건 대용량 파일을 업로드/대운로드 하는 등으로 테스트 해볼 수도 있으나, TCP의 Slow Start 같은 기능으로 정확한 측정이 어려울 수도 있습니다. 그래서 보통은 회선의 Bandwidth를 회선 업체에 문의하여 확인합니다. 그 후 A와 B 사이에 거치게 되는 회선 중 가장 Bandwidth가 낮은 회선을 A와 B 사이의 Bandwidth로 정합니다.

#### **각 병목현상의 해결책**
1. Latency가 문제라면 A와 B 사이의 네트워크 시간을 줄일 방법을 고민해봐야 합니다. 이건 정말 여러가지인데요, 만약 서로 다른 나라에 위치해있다면 상대쪽에 가까운 곳에 캐시 서버를 두는 등으로 해결할 수 있습니다.
2. Bandwidth가 병목이 되어서 서비스에 영향이 있다면 이건 전송되는 데이터를 압축하여 전송하거나 회선을 바꾸는 수밖에 없습니다. (압축에 드는 시간이 Bandwidth에 의한 병목을 상쇄할 수 있는 수준이 되어야겠죠.)

위에 말씀 드린 내용은 조금 어려운 이야기이긴하지만.. 한번쯤 짚고 넘어가야할 부분입니다. 네트워크에서 생기는 병목이 Latency의 문제인가, Bandwidth의 문제인가.

네트워크가 문제인지 알기 위해서는 위 2가지를 염두해두고 다각도에서 분석해보셔야합니다.

- 내부 시스템에서 다른 서버, DB 사이에 생기는 네트워크가 지연되는가?
- 외부로 호출되는 API가 지연되는가?
- 사용자와 우리 서버간에 지연되는가?